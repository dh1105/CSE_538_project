{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE_538_RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f8f4a6d765844f7aa107dee3fe1d325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_466f92a7c8c443c49bc942c7d1cffc53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33e84732f5bd47bc845766c3265ebe03",
              "IPY_MODEL_ddf6b47de34a43b3ae4611051a43fca5",
              "IPY_MODEL_542cf83a4f364e22809941dee5df2290"
            ]
          }
        },
        "466f92a7c8c443c49bc942c7d1cffc53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33e84732f5bd47bc845766c3265ebe03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6157c7cac5149c6b0a2477ca9e7e6c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eff40e310efa43d3927c033387fcb781"
          }
        },
        "ddf6b47de34a43b3ae4611051a43fca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_888a3a2013af40759f6ebab990616d3c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1425744429,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1425744429,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_038cf91b8f334bb68e7acb28fde6e610"
          }
        },
        "542cf83a4f364e22809941dee5df2290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d1bdeaecdbd64d5da2c521d7b59b242a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.33G/1.33G [00:49&lt;00:00, 26.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c88d30178f9b4ca3966b325da82010ce"
          }
        },
        "d6157c7cac5149c6b0a2477ca9e7e6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eff40e310efa43d3927c033387fcb781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "888a3a2013af40759f6ebab990616d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "038cf91b8f334bb68e7acb28fde6e610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1bdeaecdbd64d5da2c521d7b59b242a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c88d30178f9b4ca3966b325da82010ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-_RYDMWz7oY",
        "outputId": "37a5494b-3099-4d8e-cebf-942a5656e7f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWpSU_TapQhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b1531d-2f98-4ee4-9e85-26c9b91d30b1"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 498 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 32.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PclA_sL9pQ-X"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"roberta-large-mnli\" #\"ishan/bert-base-uncased-mnli\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7oQAszPp1BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc65de50-475d-438f-9ff5-036a079b9a52"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49NiOpRRp1e6"
      },
      "source": [
        "# val_df = pd.read_csv(\"/content/drive/MyDrive/NLP/paraphrased_dataset.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/MyDrive/NLP/paraphrased_mismatched_dataset_v2.csv\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6axmSKlRqdZ",
        "outputId": "fdf8f902-fdcc-4bad-c7c6-41e33d790c76"
      },
      "source": [
        "val_df"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>genre</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>promptID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "      <th>sentence1dash</th>\n",
              "      <th>sentence2dash</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>75290c</td>\n",
              "      <td>75290</td>\n",
              "      <td>Your contribution helped make it possible for ...</td>\n",
              "      <td>( ( Your contribution ) ( ( helped ( make ( it...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NN contribution)) (V...</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "      <td>( ( Your contributions ) ( ( were ( of ( ( no ...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NNS contributions)) ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>133794c</td>\n",
              "      <td>133794</td>\n",
              "      <td>The answer has nothing to do with their cause,...</td>\n",
              "      <td>( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...</td>\n",
              "      <td>(ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...</td>\n",
              "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
              "      <td>( Dictionaries ( ( ( are indeed ) ( exercises ...</td>\n",
              "      <td>(ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In fact, dictionaries are exercises in bi-uniq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['contradiction', 'neutral', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3628c</td>\n",
              "      <td>3628</td>\n",
              "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
              "      <td>( We ( ( serve ( ( a ( classic ( Tuscan meal )...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a meal of Florentine terrine.</td>\n",
              "      <td>( We ( ( serve ( ( a meal ) ( of ( Florentine ...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a traditional Tuscan dinner, which in...</td>\n",
              "      <td>We serve a dinner of Florentine terrine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>89411c</td>\n",
              "      <td>89411</td>\n",
              "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
              "      <td>( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...</td>\n",
              "      <td>(ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...</td>\n",
              "      <td>Carl Newton and I have never had any other pre...</td>\n",
              "      <td>( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...</td>\n",
              "      <td>(ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>entailment</td>\n",
              "      <td>136158e</td>\n",
              "      <td>136158</td>\n",
              "      <td>I was on this earth you know, I've lived on th...</td>\n",
              "      <td>( I ( ( was ( on ( ( this earth ) ( you ( know...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...</td>\n",
              "      <td>I don't yet know the reason why I have lived o...</td>\n",
              "      <td>( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I do not yet know the reason I have lived on e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>9995</td>\n",
              "      <td>['contradiction', 'contradiction', 'neutral', ...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>47686c</td>\n",
              "      <td>47686</td>\n",
              "      <td>Do you watch that?</td>\n",
              "      <td>( ( ( Do you ) ( watch that ) ) ? )</td>\n",
              "      <td>(ROOT (SQ (VBP Do) (NP (PRP you)) (VP (VB watc...</td>\n",
              "      <td>Can you see?</td>\n",
              "      <td>( ( ( Can you ) see ) ? )</td>\n",
              "      <td>(ROOT (SQ (MD Can) (NP (PRP you)) (VP (VB see)...</td>\n",
              "      <td>Do you watch this?</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>9996</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>126829c</td>\n",
              "      <td>126829</td>\n",
              "      <td>To a Western ear, the most predictable of lang...</td>\n",
              "      <td>( ( ( ( ( To ( ( ( a ( Western ear ) ) , ) ( (...</td>\n",
              "      <td>(ROOT (SINV (S (S (PP (TO To) (NP (NP (DT a) (...</td>\n",
              "      <td>To the Western ear, the least predictable of l...</td>\n",
              "      <td>( ( To ( the ( Western ear ) ) ) ( , ( ( ( the...</td>\n",
              "      <td>(ROOT (S (PP (TO To) (NP (DT the) (JJ Western)...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>9997</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>nineeleven</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>37368c</td>\n",
              "      <td>37368</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>( ( The recorder ) ( ( ( ( captured ( ( the so...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VP (...</td>\n",
              "      <td>The recorder didn't capture any of the sounds.</td>\n",
              "      <td>( ( The recorder ) ( ( ( did n't ) ( capture (...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VBD ...</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>The recorder did not capture any of the sounds.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>9998</td>\n",
              "      <td>['neutral', 'neutral', 'entailment', 'neutral'...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>neutral</td>\n",
              "      <td>97460n</td>\n",
              "      <td>97460</td>\n",
              "      <td>That's a good attitude!</td>\n",
              "      <td>( That ( ( 's ( a ( good attitude ) ) ) ! ) )</td>\n",
              "      <td>(ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (DT a...</td>\n",
              "      <td>You feel good about this, don't you?</td>\n",
              "      <td>( ( You ( ( feel good ) ( about this ) ) ) ( ,...</td>\n",
              "      <td>(ROOT (SINV (S (NP (PRP You)) (VP (VBP feel) (...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You feel good about that, don’t you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>9999</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>8693e</td>\n",
              "      <td>8693</td>\n",
              "      <td>Bloomer (for `flower'), butter (for `ram'), or...</td>\n",
              "      <td>( ( ( ( ( ( ( ( ( ( ( Bloomer ( -LRB- ( ( for ...</td>\n",
              "      <td>(ROOT (FRAG (S (S (NP (NP (NP (NP (NNP Bloomer...</td>\n",
              "      <td>Bloomer is another word for flower, butter is ...</td>\n",
              "      <td>( ( Bloomer ( is ( ( another word ) ( for flow...</td>\n",
              "      <td>(ROOT (S (S (NP (NNP Bloomer)) (VP (VBZ is) (N...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                      sentence2dash\n",
              "0              0  ...  Your contributions were of no help with our st...\n",
              "1              1  ...  In fact, dictionaries are exercises in bi-uniq...\n",
              "2              2  ...           We serve a dinner of Florentine terrine.\n",
              "3              3  ...                                                NaN\n",
              "4              4  ...  I do not yet know the reason I have lived on e...\n",
              "...          ...  ...                                                ...\n",
              "9995        9995  ...                                                NaN\n",
              "9996        9996  ...                                                NaN\n",
              "9997        9997  ...    The recorder did not capture any of the sounds.\n",
              "9998        9998  ...               You feel good about that, don’t you?\n",
              "9999        9999  ...                                                NaN\n",
              "\n",
              "[10000 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfOjQq9XegQ_",
        "outputId": "5eca6af5-145d-4abe-dc15-645a5c2e07ca"
      },
      "source": [
        "val_df.dtypes"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0                 int64\n",
              "Unnamed: 0.1               int64\n",
              "annotator_labels          object\n",
              "genre                     object\n",
              "gold_label                object\n",
              "pairID                    object\n",
              "promptID                   int64\n",
              "sentence1                 object\n",
              "sentence1_binary_parse    object\n",
              "sentence1_parse           object\n",
              "sentence2                 object\n",
              "sentence2_binary_parse    object\n",
              "sentence2_parse           object\n",
              "sentence1dash             object\n",
              "sentence2dash             object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxT6PkF9fSxR"
      },
      "source": [
        "# val_df['sentence1'].isnull().values.any()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFMZ-jG1p-uF"
      },
      "source": [
        "val_df['sentence1'] = val_df['sentence1'].astype(str)\n",
        "val_df['sentence2'] = val_df['sentence2'].astype(str)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK1rxarXqBxR"
      },
      "source": [
        "val_df['sentence1dash'] = val_df['sentence1dash'].astype(str)\n",
        "val_df['sentence2dash'] = val_df['sentence2dash'].astype(str)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEI7TaSfqNJ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a1c5a3c-12fb-4028-9c7c-ded9667279e8"
      },
      "source": [
        "val_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>genre</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>promptID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "      <th>sentence1dash</th>\n",
              "      <th>sentence2dash</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>75290c</td>\n",
              "      <td>75290</td>\n",
              "      <td>Your contribution helped make it possible for ...</td>\n",
              "      <td>( ( Your contribution ) ( ( helped ( make ( it...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NN contribution)) (V...</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "      <td>( ( Your contributions ) ( ( were ( of ( ( no ...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NNS contributions)) ...</td>\n",
              "      <td>nan</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>133794c</td>\n",
              "      <td>133794</td>\n",
              "      <td>The answer has nothing to do with their cause,...</td>\n",
              "      <td>( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...</td>\n",
              "      <td>(ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...</td>\n",
              "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
              "      <td>( Dictionaries ( ( ( are indeed ) ( exercises ...</td>\n",
              "      <td>(ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...</td>\n",
              "      <td>nan</td>\n",
              "      <td>In fact, dictionaries are exercises in bi-uniq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['contradiction', 'neutral', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3628c</td>\n",
              "      <td>3628</td>\n",
              "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
              "      <td>( We ( ( serve ( ( a ( classic ( Tuscan meal )...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a meal of Florentine terrine.</td>\n",
              "      <td>( We ( ( serve ( ( a meal ) ( of ( Florentine ...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a traditional Tuscan dinner, which in...</td>\n",
              "      <td>We serve a dinner of Florentine terrine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>89411c</td>\n",
              "      <td>89411</td>\n",
              "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
              "      <td>( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...</td>\n",
              "      <td>(ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...</td>\n",
              "      <td>Carl Newton and I have never had any other pre...</td>\n",
              "      <td>( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...</td>\n",
              "      <td>(ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>entailment</td>\n",
              "      <td>136158e</td>\n",
              "      <td>136158</td>\n",
              "      <td>I was on this earth you know, I've lived on th...</td>\n",
              "      <td>( I ( ( was ( on ( ( this earth ) ( you ( know...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...</td>\n",
              "      <td>I don't yet know the reason why I have lived o...</td>\n",
              "      <td>( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...</td>\n",
              "      <td>nan</td>\n",
              "      <td>I do not yet know the reason I have lived on e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>9995</td>\n",
              "      <td>['contradiction', 'contradiction', 'neutral', ...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>47686c</td>\n",
              "      <td>47686</td>\n",
              "      <td>Do you watch that?</td>\n",
              "      <td>( ( ( Do you ) ( watch that ) ) ? )</td>\n",
              "      <td>(ROOT (SQ (VBP Do) (NP (PRP you)) (VP (VB watc...</td>\n",
              "      <td>Can you see?</td>\n",
              "      <td>( ( ( Can you ) see ) ? )</td>\n",
              "      <td>(ROOT (SQ (MD Can) (NP (PRP you)) (VP (VB see)...</td>\n",
              "      <td>Do you watch this?</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>9996</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>126829c</td>\n",
              "      <td>126829</td>\n",
              "      <td>To a Western ear, the most predictable of lang...</td>\n",
              "      <td>( ( ( ( ( To ( ( ( a ( Western ear ) ) , ) ( (...</td>\n",
              "      <td>(ROOT (SINV (S (S (PP (TO To) (NP (NP (DT a) (...</td>\n",
              "      <td>To the Western ear, the least predictable of l...</td>\n",
              "      <td>( ( To ( the ( Western ear ) ) ) ( , ( ( ( the...</td>\n",
              "      <td>(ROOT (S (PP (TO To) (NP (DT the) (JJ Western)...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>9997</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>nineeleven</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>37368c</td>\n",
              "      <td>37368</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>( ( The recorder ) ( ( ( ( captured ( ( the so...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VP (...</td>\n",
              "      <td>The recorder didn't capture any of the sounds.</td>\n",
              "      <td>( ( The recorder ) ( ( ( did n't ) ( capture (...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VBD ...</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>The recorder did not capture any of the sounds.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>9998</td>\n",
              "      <td>['neutral', 'neutral', 'entailment', 'neutral'...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>neutral</td>\n",
              "      <td>97460n</td>\n",
              "      <td>97460</td>\n",
              "      <td>That's a good attitude!</td>\n",
              "      <td>( That ( ( 's ( a ( good attitude ) ) ) ! ) )</td>\n",
              "      <td>(ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (DT a...</td>\n",
              "      <td>You feel good about this, don't you?</td>\n",
              "      <td>( ( You ( ( feel good ) ( about this ) ) ) ( ,...</td>\n",
              "      <td>(ROOT (SINV (S (NP (PRP You)) (VP (VBP feel) (...</td>\n",
              "      <td>nan</td>\n",
              "      <td>You feel good about that, don’t you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>9999</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>8693e</td>\n",
              "      <td>8693</td>\n",
              "      <td>Bloomer (for `flower'), butter (for `ram'), or...</td>\n",
              "      <td>( ( ( ( ( ( ( ( ( ( ( Bloomer ( -LRB- ( ( for ...</td>\n",
              "      <td>(ROOT (FRAG (S (S (NP (NP (NP (NP (NNP Bloomer...</td>\n",
              "      <td>Bloomer is another word for flower, butter is ...</td>\n",
              "      <td>( ( Bloomer ( is ( ( another word ) ( for flow...</td>\n",
              "      <td>(ROOT (S (S (NP (NNP Bloomer)) (VP (VBZ is) (N...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                      sentence2dash\n",
              "0              0  ...  Your contributions were of no help with our st...\n",
              "1              1  ...  In fact, dictionaries are exercises in bi-uniq...\n",
              "2              2  ...           We serve a dinner of Florentine terrine.\n",
              "3              3  ...                                                nan\n",
              "4              4  ...  I do not yet know the reason I have lived on e...\n",
              "...          ...  ...                                                ...\n",
              "9995        9995  ...                                                nan\n",
              "9996        9996  ...                                                nan\n",
              "9997        9997  ...    The recorder did not capture any of the sounds.\n",
              "9998        9998  ...               You feel good about that, don’t you?\n",
              "9999        9999  ...                                                nan\n",
              "\n",
              "[10000 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWnLxCLXvF-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c075bf19-e231-4627-ab6b-ae94e348357d"
      },
      "source": [
        "val_df = val_df[val_df['gold_label'] != '-']\n",
        "val_df"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>genre</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>promptID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "      <th>sentence1dash</th>\n",
              "      <th>sentence2dash</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>75290c</td>\n",
              "      <td>75290</td>\n",
              "      <td>Your contribution helped make it possible for ...</td>\n",
              "      <td>( ( Your contribution ) ( ( helped ( make ( it...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NN contribution)) (V...</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "      <td>( ( Your contributions ) ( ( were ( of ( ( no ...</td>\n",
              "      <td>(ROOT (S (NP (PRP$ Your) (NNS contributions)) ...</td>\n",
              "      <td>nan</td>\n",
              "      <td>Your contributions were of no help with our st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>133794c</td>\n",
              "      <td>133794</td>\n",
              "      <td>The answer has nothing to do with their cause,...</td>\n",
              "      <td>( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...</td>\n",
              "      <td>(ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...</td>\n",
              "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
              "      <td>( Dictionaries ( ( ( are indeed ) ( exercises ...</td>\n",
              "      <td>(ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...</td>\n",
              "      <td>nan</td>\n",
              "      <td>In fact, dictionaries are exercises in bi-uniq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['contradiction', 'neutral', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3628c</td>\n",
              "      <td>3628</td>\n",
              "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
              "      <td>( We ( ( serve ( ( a ( classic ( Tuscan meal )...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a meal of Florentine terrine.</td>\n",
              "      <td>( We ( ( serve ( ( a meal ) ( of ( Florentine ...</td>\n",
              "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
              "      <td>We serve a traditional Tuscan dinner, which in...</td>\n",
              "      <td>We serve a dinner of Florentine terrine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>letters</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>89411c</td>\n",
              "      <td>89411</td>\n",
              "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
              "      <td>( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...</td>\n",
              "      <td>(ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...</td>\n",
              "      <td>Carl Newton and I have never had any other pre...</td>\n",
              "      <td>( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...</td>\n",
              "      <td>(ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>entailment</td>\n",
              "      <td>136158e</td>\n",
              "      <td>136158</td>\n",
              "      <td>I was on this earth you know, I've lived on th...</td>\n",
              "      <td>( I ( ( was ( on ( ( this earth ) ( you ( know...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...</td>\n",
              "      <td>I don't yet know the reason why I have lived o...</td>\n",
              "      <td>( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...</td>\n",
              "      <td>(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...</td>\n",
              "      <td>nan</td>\n",
              "      <td>I do not yet know the reason I have lived on e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>9995</td>\n",
              "      <td>['contradiction', 'contradiction', 'neutral', ...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>47686c</td>\n",
              "      <td>47686</td>\n",
              "      <td>Do you watch that?</td>\n",
              "      <td>( ( ( Do you ) ( watch that ) ) ? )</td>\n",
              "      <td>(ROOT (SQ (VBP Do) (NP (PRP you)) (VP (VB watc...</td>\n",
              "      <td>Can you see?</td>\n",
              "      <td>( ( ( Can you ) see ) ? )</td>\n",
              "      <td>(ROOT (SQ (MD Can) (NP (PRP you)) (VP (VB see)...</td>\n",
              "      <td>Do you watch this?</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>9996</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>126829c</td>\n",
              "      <td>126829</td>\n",
              "      <td>To a Western ear, the most predictable of lang...</td>\n",
              "      <td>( ( ( ( ( To ( ( ( a ( Western ear ) ) , ) ( (...</td>\n",
              "      <td>(ROOT (SINV (S (S (PP (TO To) (NP (NP (DT a) (...</td>\n",
              "      <td>To the Western ear, the least predictable of l...</td>\n",
              "      <td>( ( To ( the ( Western ear ) ) ) ( , ( ( ( the...</td>\n",
              "      <td>(ROOT (S (PP (TO To) (NP (DT the) (JJ Western)...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>9997</td>\n",
              "      <td>['contradiction', 'contradiction', 'contradict...</td>\n",
              "      <td>nineeleven</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>37368c</td>\n",
              "      <td>37368</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>( ( The recorder ) ( ( ( ( captured ( ( the so...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VP (...</td>\n",
              "      <td>The recorder didn't capture any of the sounds.</td>\n",
              "      <td>( ( The recorder ) ( ( ( did n't ) ( capture (...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN recorder)) (VP (VBD ...</td>\n",
              "      <td>The recorder captured the sounds of loud thump...</td>\n",
              "      <td>The recorder did not capture any of the sounds.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>9998</td>\n",
              "      <td>['neutral', 'neutral', 'entailment', 'neutral'...</td>\n",
              "      <td>facetoface</td>\n",
              "      <td>neutral</td>\n",
              "      <td>97460n</td>\n",
              "      <td>97460</td>\n",
              "      <td>That's a good attitude!</td>\n",
              "      <td>( That ( ( 's ( a ( good attitude ) ) ) ! ) )</td>\n",
              "      <td>(ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (DT a...</td>\n",
              "      <td>You feel good about this, don't you?</td>\n",
              "      <td>( ( You ( ( feel good ) ( about this ) ) ) ( ,...</td>\n",
              "      <td>(ROOT (SINV (S (NP (PRP You)) (VP (VBP feel) (...</td>\n",
              "      <td>nan</td>\n",
              "      <td>You feel good about that, don’t you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>9999</td>\n",
              "      <td>['entailment', 'entailment', 'entailment', 'en...</td>\n",
              "      <td>verbatim</td>\n",
              "      <td>entailment</td>\n",
              "      <td>8693e</td>\n",
              "      <td>8693</td>\n",
              "      <td>Bloomer (for `flower'), butter (for `ram'), or...</td>\n",
              "      <td>( ( ( ( ( ( ( ( ( ( ( Bloomer ( -LRB- ( ( for ...</td>\n",
              "      <td>(ROOT (FRAG (S (S (NP (NP (NP (NP (NNP Bloomer...</td>\n",
              "      <td>Bloomer is another word for flower, butter is ...</td>\n",
              "      <td>( ( Bloomer ( is ( ( another word ) ( for flow...</td>\n",
              "      <td>(ROOT (S (S (NP (NNP Bloomer)) (VP (VBZ is) (N...</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9832 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                      sentence2dash\n",
              "0              0  ...  Your contributions were of no help with our st...\n",
              "1              1  ...  In fact, dictionaries are exercises in bi-uniq...\n",
              "2              2  ...           We serve a dinner of Florentine terrine.\n",
              "3              3  ...                                                nan\n",
              "4              4  ...  I do not yet know the reason I have lived on e...\n",
              "...          ...  ...                                                ...\n",
              "9995        9995  ...                                                nan\n",
              "9996        9996  ...                                                nan\n",
              "9997        9997  ...    The recorder did not capture any of the sounds.\n",
              "9998        9998  ...               You feel good about that, don’t you?\n",
              "9999        9999  ...                                                nan\n",
              "\n",
              "[9832 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1nxJCEaqRMh"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class MNLIDataRoberta(Dataset):\n",
        "\n",
        "  def __init__(self, val_df):\n",
        "    self.label_dict = {'entailment': 2, 'contradiction': 0, 'neutral': 1}\n",
        "\n",
        "    # self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "\n",
        "    self.base_path = '/content/'\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # self.train_data = None\n",
        "    self.val_data = None\n",
        "    self.paraphrased_data = None\n",
        "    self.init_data()\n",
        "\n",
        "  def get_tokenizer(self):\n",
        "    return self.tokenizer\n",
        "\n",
        "  def init_data(self):\n",
        "    # Saving takes too much RAM\n",
        "    #\n",
        "    # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
        "    #   print(\"Found training data\")\n",
        "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
        "    #     self.train_data = pickle.load(f)\n",
        "    # else:\n",
        "    #   self.train_data = self.load_data(self.train_df)\n",
        "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
        "    #     pickle.dump(self.train_data, f)\n",
        "    # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
        "    #   print(\"Found val data\")\n",
        "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
        "    #     self.val_data = pickle.load(f)\n",
        "    # else:\n",
        "    #   self.val_data = self.load_data(self.val_df)\n",
        "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
        "    #     pickle.dump(self.val_data, f)\n",
        "    # self.train_data = self.load_data(self.train_df)\n",
        "    self.val_data = self.load_data(self.val_df)\n",
        "    self.paraphrased_data = self.load_paraphrased_data(self.val_df)\n",
        "\n",
        "  def load_paraphrased_data(self, df):\n",
        "    MAX_LEN = 512\n",
        "    token_ids = []\n",
        "    mask_ids = []\n",
        "    seg_ids = []\n",
        "    y = []\n",
        "\n",
        "    premise_list = df['sentence1'].to_list()\n",
        "    paraphrased_premise_list = df['sentence1dash'].to_list()\n",
        "    hypothesis_list = df['sentence2'].to_list()\n",
        "    paraphrased_hypothesis_list = df['sentence2dash'].to_list()\n",
        "    label_list = df['gold_label'].to_list()\n",
        "\n",
        "    for (premise, para_premise, hypothesis, para_hypothesis, label) in zip(premise_list, paraphrased_premise_list, hypothesis_list, paraphrased_hypothesis_list, label_list):\n",
        "\n",
        "      # print(premise, para_premise, hypothesis, para_hypothesis)\n",
        "      if para_hypothesis != 'nan':\n",
        "        encoded_values = self.tokenizer.encode_plus(premise, para_hypothesis, return_token_type_ids=True, return_attention_mask=True)\n",
        "        token_ids.append(torch.tensor(encoded_values['input_ids']))\n",
        "        seg_ids.append(torch.tensor(encoded_values['token_type_ids']))\n",
        "        mask_ids.append(torch.tensor(encoded_values['attention_mask']))\n",
        "        y.append(self.label_dict[label])\n",
        "\n",
        "      if para_premise != 'nan':\n",
        "        encoded_values = self.tokenizer.encode_plus(para_premise, hypothesis, return_token_type_ids=True, return_attention_mask=True)\n",
        "        token_ids.append(torch.tensor(encoded_values['input_ids']))\n",
        "        seg_ids.append(torch.tensor(encoded_values['token_type_ids']))\n",
        "        mask_ids.append(torch.tensor(encoded_values['attention_mask']))\n",
        "        y.append(self.label_dict[label])\n",
        "\n",
        "      if para_hypothesis != 'nan' and para_premise != 'nan':\n",
        "        encoded_values = self.tokenizer.encode_plus(para_premise, para_hypothesis, return_token_type_ids=True, return_attention_mask=True)\n",
        "        token_ids.append(torch.tensor(encoded_values['input_ids']))\n",
        "        seg_ids.append(torch.tensor(encoded_values['token_type_ids']))\n",
        "        mask_ids.append(torch.tensor(encoded_values['attention_mask']))\n",
        "        y.append(self.label_dict[label])\n",
        "\n",
        "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
        "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
        "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
        "    y = torch.tensor(y)\n",
        "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
        "    print(len(dataset))\n",
        "    return dataset\n",
        "\n",
        "  def load_data(self, df):\n",
        "    MAX_LEN = 512\n",
        "    token_ids = []\n",
        "    mask_ids = []\n",
        "    seg_ids = []\n",
        "    y = []\n",
        "\n",
        "    premise_list = df['sentence1'].to_list()\n",
        "    hypothesis_list = df['sentence2'].to_list()\n",
        "    label_list = df['gold_label'].to_list()\n",
        "\n",
        "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
        "      # premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
        "      # hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
        "      # pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
        "      # premise_len = len(premise_id)\n",
        "      # hypothesis_len = len(hypothesis_id)\n",
        "\n",
        "      # segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
        "      # attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
        "      # print(premise, hypothesis)\n",
        "      if premise != 'nan' and hypothesis != 'nan':\n",
        "        encoded_values = self.tokenizer.encode_plus(premise, hypothesis, return_token_type_ids=True, return_attention_mask=True)\n",
        "        token_ids.append(torch.tensor(encoded_values['input_ids']))\n",
        "        seg_ids.append(torch.tensor(encoded_values['token_type_ids']))\n",
        "        mask_ids.append(torch.tensor(encoded_values['attention_mask']))\n",
        "        y.append(self.label_dict[label])\n",
        "    \n",
        "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
        "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
        "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
        "    y = torch.tensor(y)\n",
        "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
        "    print(len(dataset))\n",
        "    return dataset\n",
        "\n",
        "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
        "    # train_loader = DataLoader(\n",
        "    #   self.train_data,\n",
        "    #   shuffle=shuffle,\n",
        "    #   batch_size=batch_size\n",
        "    # )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "      self.val_data,\n",
        "      shuffle=shuffle,\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    paraphrased_loader = DataLoader(\n",
        "        self.paraphrased_data,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return val_loader, paraphrased_loader"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4iGUYmFsRLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b0c5b9-298f-4b24-8bc5-3e30fcaf8dde"
      },
      "source": [
        "mnli_dataset = MNLIDataRoberta(val_df)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9832\n",
            "8165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4En5EZPtjxD"
      },
      "source": [
        "val_loader, paraphrased_loader = mnli_dataset.get_data_loaders(batch_size=32)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uXwVbMOsWg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bebaa5-1982-4138-d2d3-d088aa6c6ed9"
      },
      "source": [
        "print(len(paraphrased_loader), len(val_loader))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256 308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldwlu4B7sZLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f8f4a6d765844f7aa107dee3fe1d325",
            "466f92a7c8c443c49bc942c7d1cffc53",
            "33e84732f5bd47bc845766c3265ebe03",
            "ddf6b47de34a43b3ae4611051a43fca5",
            "542cf83a4f364e22809941dee5df2290",
            "d6157c7cac5149c6b0a2477ca9e7e6c1",
            "eff40e310efa43d3927c033387fcb781",
            "888a3a2013af40759f6ebab990616d3c",
            "038cf91b8f334bb68e7acb28fde6e610",
            "d1bdeaecdbd64d5da2c521d7b59b242a",
            "c88d30178f9b4ca3966b325da82010ce"
          ]
        },
        "outputId": "3de749b1-966d-4c4e-a8cf-82ad9d2b683f"
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.to(device)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f8f4a6d765844f7aa107dee3fe1d325",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RepVoDxsm6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97d7f12-1f70-4b6b-9099-15f4815e6a0c"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 355,362,819 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH7kauE1sugn"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABKIbqmwlI4"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  # print(pred_flat)\n",
        "  # print(labels_flat)\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG8VU1OctG5x"
      },
      "source": [
        "model.eval()\n",
        "total_val_acc  = 0\n",
        "total_val_loss = 0\n",
        "total_val_acc_para = 0\n",
        "total_val_loss_para = 0\n",
        "# output_orig = {}\n",
        "# output_predict = {}\n",
        "# ind = 0\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
        "    # optimizer.zero_grad()\n",
        "    pair_token_ids = pair_token_ids.to(device)\n",
        "    mask_ids = mask_ids.to(device)\n",
        "    seg_ids = seg_ids.to(device)\n",
        "    labels = y.to(device)\n",
        "\n",
        "    # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
        "    result = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels,\n",
        "                             return_dict=True)\n",
        "  \n",
        "    loss = result.loss\n",
        "    logits = result.logits\n",
        "    # print(logits.shape)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "        \n",
        "        # loss = criterion(prediction, labels)\n",
        "    acc = flat_accuracy(logits, label_ids)\n",
        "    # print(acc, loss)\n",
        "\n",
        "    total_val_loss += loss.item()\n",
        "    total_val_acc  += acc\n",
        "  \n",
        "  for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(paraphrased_loader):\n",
        "    # optimizer.zero_grad()\n",
        "    pair_token_ids = pair_token_ids.to(device)\n",
        "    mask_ids = mask_ids.to(device)\n",
        "    seg_ids = seg_ids.to(device)\n",
        "    labels = y.to(device)\n",
        "\n",
        "    result = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels,\n",
        "                             return_dict=True)\n",
        "  \n",
        "    loss = result.loss\n",
        "    logits = result.logits\n",
        "    # print(logits.shape)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "    # pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        \n",
        "        # loss = criterion(prediction, labels)\n",
        "    acc = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    total_val_loss_para += loss.item()\n",
        "    total_val_acc_para  += acc\n",
        "    # output_predict[ind] = {'input_ids':pair_token_ids, 'label_real': label_ids[0], 'label_predict':pred_flat[0]}\n",
        "    # ind += 1\n",
        "\n",
        "val_acc  = total_val_acc/len(val_loader)\n",
        "val_loss = total_val_loss/len(val_loader)\n",
        "val_acc_para = total_val_acc_para/len(paraphrased_loader)\n",
        "val_loss_para = total_val_loss_para/len(paraphrased_loader)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiVT8ZshgdX9",
        "outputId": "1841cc87-ccf9-4a5a-e4a9-45c1913a0b00"
      },
      "source": [
        "print(f'val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} | val_loss_para: {val_loss_para:.4f} val_acc: {val_acc_para:.4f}')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss: 0.2878 val_acc: 0.9015 | val_loss_para: 0.3129 val_acc: 0.8935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kyirg08NJwv",
        "outputId": "943931bb-1b0c-491d-d12a-e76fbb823582"
      },
      "source": [
        "print(f'val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} | val_loss_para: {val_loss_para:.4f} val_acc: {val_acc_para:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss: 0.2758 val_acc: 0.9035 | val_loss_para: 0.2968 val_acc: 0.8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYY_SYLyuQZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0768f98a-0bd2-4cd1-fb0d-11e6558abd9d"
      },
      "source": [
        "print(f'val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} | val_loss_para: {val_loss_para:.4f} val_acc: {val_acc_para:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss: 0.2893 val_acc: 0.9012 | val_loss_para: 0.6543 val_acc: 0.7630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxJuln-tMbBH"
      },
      "source": [
        "eval_df = pd.DataFrame([],columns=['sentence', 'label', 'pred'])\n",
        "tokenizer = mnli_dataset.get_tokenizer()\n",
        "for item in list(output_predict.values()):\n",
        "    sent = tokenizer.decode(item['input_ids'][0],skip_special_tokens=False)\n",
        "    print(sent)\n",
        "    pred = item['label_predict']\n",
        "    label = item['label_real']\n",
        "    new_row = {'sentence':sent,'label':label, 'pred': pred}\n",
        "    eval_df = eval_df.append(new_row, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3doPyQ4M1B-"
      },
      "source": [
        "eval_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HMRPwP8NuG9"
      },
      "source": [
        "eval_df.to_csv(\"/content/drive/MyDrive/NLP/Project/eval_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL67WjFWRHXa"
      },
      "source": [
        "wrong_preds = eval_df[eval_df['label'] != eval_df['pred']]\n",
        "wrong_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRpojydqRRHz"
      },
      "source": [
        "wrong_preds.to_csv(\"/content/drive/MyDrive/NLP/Project/wrong_preds.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GjB99zA2goo"
      },
      "source": [
        "print(f'val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} | val_loss_para: {val_loss_para:.4f} val_acc_para: {val_acc_para:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}